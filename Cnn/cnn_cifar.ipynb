{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Projekt-R-DU/prva-faza/blob/master/Cnn/cnn_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AjnuEKRoBoY"
      },
      "source": [
        "Downloading and unpacking the CIFAR dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4mqfnqCnXAf",
        "outputId": "eba41855-ae95-4538-cec2-a9b77a4fa891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'cifar-10-python*': No such file or directory\n",
            "--2021-12-16 15:33:12--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  76.6MB/s    in 2.1s    \n",
            "\n",
            "2021-12-16 15:33:14 (76.6 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-python.tar.gz\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!rm cifar-10-python*\n",
        "!wget \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA3WcTMBnhlN",
        "outputId": "6a4650f4-e4de-49f2-fa1f-e069de3f3e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "cifar-10-batches-py  cifar-10-python.tar.gz  sample_data\n"
          ]
        }
      ],
      "source": [
        "!tar -xvzf cifar-10-python.tar.gz\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVGWr_HIoUe8"
      },
      "source": [
        "Preparing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mhxj1jKnzaG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def shuffle_data(data_x, data_y):\n",
        "  indices = np.arange(data_x.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
        "  shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
        "  return shuffled_data_x, shuffled_data_y\n",
        "\n",
        "def unpickle(file):\n",
        "  fo = open(file, 'rb')\n",
        "  dict = pickle.load(fo, encoding='latin1')\n",
        "  fo.close()\n",
        "  return dict\n",
        "\n",
        "DATA_DIR = 'cifar-10-batches-py/'\n",
        "\n",
        "img_height = 32\n",
        "img_width = 32\n",
        "num_channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
        "train_y = []\n",
        "for i in range(1, 6):\n",
        "  subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
        "  train_x = np.vstack((train_x, subset['data']))\n",
        "  train_y += subset['labels']\n",
        "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0, 2, 3, 1)\n",
        "train_y = np.array(train_y, dtype=np.int32)\n",
        "\n",
        "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
        "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0, 2, 3, 1).astype(np.float32)\n",
        "test_y = np.array(subset['labels'], dtype=np.int32)\n",
        "\n",
        "valid_size = 5000\n",
        "train_x, train_y = shuffle_data(train_x, train_y)\n",
        "valid_x = train_x[:valid_size, ...]\n",
        "valid_y = train_y[:valid_size, ...]\n",
        "train_x = train_x[valid_size:, ...]\n",
        "train_y = train_y[valid_size:, ...]\n",
        "data_mean = train_x.mean((0, 1, 2))\n",
        "data_std = train_x.std((0, 1, 2))\n",
        "\n",
        "train_x = (train_x - data_mean) / data_std\n",
        "valid_x = (valid_x - data_mean) / data_std\n",
        "test_x = (test_x - data_mean) / data_std\n",
        "\n",
        "train_x = train_x.transpose(0, 3, 1, 2)\n",
        "valid_x = valid_x.transpose(0, 3, 1, 2)\n",
        "test_x = test_x.transpose(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "\n",
        "valid_values = []\n",
        "\n",
        "for i in valid_y:\n",
        "  appended = np.zeros(10)\n",
        "  appended[i] = 1\n",
        "  valid_values.append(appended)\n",
        "\n",
        "valid_labels = np.array(valid_values)\n",
        "\n",
        "train_values = []\n",
        "\n",
        "for i in train_y:\n",
        "  appended = np.zeros(10)\n",
        "  appended[i] = 1\n",
        "  train_values.append(appended)\n",
        "train_labels = np.array(train_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNATlAQNokhv"
      },
      "source": [
        "Defining our convolutional neural network as\n",
        "\n",
        "```\n",
        "conv(16,5) -> relu() -> pool(3,2) -> conv(32,5) -> relu() -> pool(3,2) -> fc(256) -> relu() -> fc(128) -> relu() -> fc(10)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pSmXiMVoeu1"
      },
      "outputs": [],
      "source": [
        "from numpy import floor\n",
        "from torch import nn\n",
        "\n",
        "class ConvolutionalModel(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, in_width, conv1_channels, pool1_width, conv2_channels, pool2_width, fc3_width, fc4_width, class_count):\n",
        "        super(ConvolutionalModel, self).__init__()\n",
        "                                                                                                                # in_channels x in_width x in_width\n",
        "        self.conv1 = nn.Conv2d(in_channels, conv1_channels, kernel_size=5, stride=1, padding=2, bias=True)      # conv1_channels x in_width x in_width\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(pool1_width, stride=2)                                                        # conv1_channels x w2 x w2\n",
        "\n",
        "        w2 = floor((in_width - pool1_width) / 2 + 1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, kernel_size=5, stride=1, padding=2, bias=True)   # conv2_channels x w2 x w2\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(pool2_width, stride=2)                                                        # conv2_channels x w3 x w3\n",
        "        \n",
        "        w3 = floor((w2 - pool2_width) / 2 + 1)\n",
        "\n",
        "        self.flatten3 = nn.Flatten()                                                                            # (conv2_channels x w3 x w3)\n",
        "        self.fc3 = nn.Linear((int)(conv2_channels * w3 * w3), fc3_width)                                        # fc3width\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(fc3_width, fc4_width)                                                              # fc4width\n",
        "        self.relu4 = nn.ReLU()                                                       \n",
        "\n",
        "        self.fc_logits = nn.Linear(fc4_width, class_count)                                                      # class_count\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear) and m is not self.fc_logits:\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        self.fc_logits.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv1(x)\n",
        "        h = self.relu1(h)\n",
        "        h = self.pool1(h)\n",
        "\n",
        "        h = self.conv2(h)\n",
        "        h = self.relu2(h)\n",
        "        h = self.pool2(h)\n",
        "        \n",
        "        h = self.flatten3(h)\n",
        "        h = self.fc3(h)\n",
        "        h = self.relu3(h)\n",
        "\n",
        "        h = self.fc4(h)\n",
        "        h = self.relu4(h)\n",
        "\n",
        "        logits = self.fc_logits(h)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAWjUKlVqbcJ"
      },
      "source": [
        "Evaluation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoGYDO4lqkJl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def evaluate(model, criterion, x_in, y_in):\n",
        "  # np array u tensor\n",
        "  x = torch.FloatTensor(x_in)\n",
        "  y = torch.FloatTensor(y_in)\n",
        "\n",
        "  outputs = model.forward(x)\n",
        "  loss = criterion(outputs, y)\n",
        "\n",
        "  total = len(y)\n",
        "  temp_list = []\n",
        "  for test in y:\n",
        "    temp_list.append(np.where(test==1)[0][0])\n",
        "\n",
        "  targets_np = np.array(temp_list)\n",
        "  _, predicted_tensor = outputs.max(1)\n",
        "  predicted_np = predicted_tensor.detach().numpy()\n",
        "  correct = np.sum(targets_np==predicted_np)\n",
        "  accuracy = correct / total\n",
        "\n",
        "  return float(loss.detach().numpy()), accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYgIoiU0rLV7"
      },
      "source": [
        "Our loss functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ygSgIHMrInC"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display_markdown\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NegativeLogLikelihood(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(NegativeLogLikelihood, self).__init__()\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max = torch.max(x, dim=1, keepdim=True)[0]\n",
        "        x_exp_shifted = torch.exp(x - max)\n",
        "        return x_exp_shifted / torch.sum(x_exp_shifted, dim=1, keepdim=True)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #print('before softmax', x)\n",
        "        probs = self.softmax(x)\n",
        "        #print('after softmax', probs)\n",
        "        #print(probs * y) \n",
        "        return -torch.mean(torch.log(torch.sum(probs * y, dim=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bit3lAqysfZ9"
      },
      "source": [
        "Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Lb0m0rsn6I",
        "outputId": "57675ac3-ade7-44ff-d446-cc7f8be36183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0/500, batch_loss: 2.565282106399536\n",
            "epoch: 0, step: 50/500, batch_loss: 1.8628178834915161\n",
            "epoch: 0, step: 100/500, batch_loss: 1.6096713542938232\n",
            "epoch: 0, step: 150/500, batch_loss: 1.4577616453170776\n",
            "epoch: 0, step: 200/500, batch_loss: 1.43641996383667\n",
            "epoch: 0, step: 250/500, batch_loss: 1.3916767835617065\n",
            "epoch: 0, step: 300/500, batch_loss: 1.270956039428711\n",
            "epoch: 0, step: 350/500, batch_loss: 1.4761244058609009\n",
            "epoch: 0, step: 400/500, batch_loss: 1.1272298097610474\n",
            "epoch: 0, step: 450/500, batch_loss: 1.3219292163848877\n",
            "valid_loss:1.2068605422973633 , valid_accuracy:0.5666\n",
            "epoch: 1, step: 0/500, batch_loss: 1.2272288799285889\n",
            "epoch: 1, step: 50/500, batch_loss: 1.1870700120925903\n",
            "epoch: 1, step: 100/500, batch_loss: 1.1687227487564087\n",
            "epoch: 1, step: 150/500, batch_loss: 1.0749213695526123\n",
            "epoch: 1, step: 200/500, batch_loss: 1.2742105722427368\n",
            "epoch: 1, step: 250/500, batch_loss: 1.1828722953796387\n",
            "epoch: 1, step: 300/500, batch_loss: 0.9947695136070251\n",
            "epoch: 1, step: 350/500, batch_loss: 0.878267228603363\n",
            "epoch: 1, step: 400/500, batch_loss: 1.153747320175171\n",
            "epoch: 1, step: 450/500, batch_loss: 1.0093662738800049\n",
            "valid_loss:1.0808188915252686 , valid_accuracy:0.6186\n",
            "epoch: 2, step: 0/500, batch_loss: 0.9710072875022888\n",
            "epoch: 2, step: 50/500, batch_loss: 0.9960668683052063\n",
            "epoch: 2, step: 100/500, batch_loss: 1.1708306074142456\n",
            "epoch: 2, step: 150/500, batch_loss: 0.7835173606872559\n",
            "epoch: 2, step: 200/500, batch_loss: 0.8737477660179138\n",
            "epoch: 2, step: 250/500, batch_loss: 1.0091893672943115\n",
            "epoch: 2, step: 300/500, batch_loss: 0.774605393409729\n",
            "epoch: 2, step: 350/500, batch_loss: 0.8339200615882874\n",
            "epoch: 2, step: 400/500, batch_loss: 0.914758563041687\n",
            "epoch: 2, step: 450/500, batch_loss: 0.848875105381012\n",
            "valid_loss:0.960077166557312 , valid_accuracy:0.667\n",
            "epoch: 3, step: 0/500, batch_loss: 0.6516202688217163\n",
            "epoch: 3, step: 50/500, batch_loss: 0.7775517106056213\n",
            "epoch: 3, step: 100/500, batch_loss: 0.8755242228507996\n",
            "epoch: 3, step: 150/500, batch_loss: 0.9351415634155273\n",
            "epoch: 3, step: 200/500, batch_loss: 0.8052926063537598\n",
            "epoch: 3, step: 250/500, batch_loss: 0.8976227641105652\n",
            "epoch: 3, step: 300/500, batch_loss: 0.8469353318214417\n",
            "epoch: 3, step: 350/500, batch_loss: 0.7445039749145508\n",
            "epoch: 3, step: 400/500, batch_loss: 0.745453417301178\n",
            "epoch: 3, step: 450/500, batch_loss: 0.7814764976501465\n",
            "valid_loss:0.9352452158927917 , valid_accuracy:0.6776\n",
            "epoch: 4, step: 0/500, batch_loss: 0.6770473718643188\n",
            "epoch: 4, step: 50/500, batch_loss: 0.5867014527320862\n",
            "epoch: 4, step: 100/500, batch_loss: 0.43735402822494507\n",
            "epoch: 4, step: 150/500, batch_loss: 0.63251793384552\n",
            "epoch: 4, step: 200/500, batch_loss: 0.7350287437438965\n",
            "epoch: 4, step: 250/500, batch_loss: 0.7441996932029724\n",
            "epoch: 4, step: 300/500, batch_loss: 0.9427374005317688\n",
            "epoch: 4, step: 350/500, batch_loss: 0.6624249219894409\n",
            "epoch: 4, step: 400/500, batch_loss: 0.9453920722007751\n",
            "epoch: 4, step: 450/500, batch_loss: 0.7250591516494751\n",
            "valid_loss:0.9051445126533508 , valid_accuracy:0.6824\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "plot_data = {}\n",
        "plot_data['train_loss'] = []\n",
        "plot_data['valid_loss'] = []\n",
        "plot_data['train_acc'] = []\n",
        "plot_data['valid_acc'] = []\n",
        "plot_data['lr'] = []\n",
        "\n",
        "data_channels = 3\n",
        "data_width = 32\n",
        "CifarCnn = ConvolutionalModel(data_channels, data_width, 16, 3, 32, 3, 256, 128, 10)\n",
        "\n",
        "criterion = NegativeLogLikelihood()\n",
        "optimizer = optim.SGD(CifarCnn.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "num_epochs = 5\n",
        "n_batch=500 #bsz izračunato iz duljine trening seta i broja batcheva\n",
        "bsz = len(train_x) // n_batch\n",
        "for epoch in range(num_epochs):\n",
        "    X, Yoh = shuffle_data(train_x, train_labels)\n",
        "    X = torch.FloatTensor(X)\n",
        "    Yoh = torch.FloatTensor(Yoh)\n",
        "    for batch in range(n_batch):\n",
        "        # broj primjera djeljiv s veličinom grupe bsz\n",
        "        batch_X = X[batch*bsz:(batch+1)*bsz, :]\n",
        "        batch_Yoh = Yoh[batch*bsz:(batch+1)*bsz, :]\n",
        "        outputs = CifarCnn.forward(batch_X)\n",
        "\n",
        "        loss = criterion(outputs, batch_Yoh)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch%50 == 0:\n",
        "            print(\"epoch: {}, step: {}/{}, batch_loss: {}\".format(epoch, batch, n_batch, loss))\n",
        "\n",
        "    valid_loss, valid_accuracy = evaluate(CifarCnn, criterion, valid_x, valid_labels)\n",
        "    print(f\"valid_loss:{valid_loss} , valid_accuracy:{valid_accuracy}\")\n",
        "    \n",
        "    lr_scheduler.step()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_cifar.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}