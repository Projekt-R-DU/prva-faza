{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Projekt-R-DU/prva-faza/blob/master/Cnn/cnn_cifarAdam512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BZeN9_23TjPd"
   },
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "from torch import nn\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, in_width, conv1_channels, pool1_width, conv2_channels, pool2_width, fc3_width, fc4_width, class_count):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "                                                                                                                # in_channels x in_width x in_width\n",
    "        self.conv1 = nn.Conv2d(in_channels, conv1_channels, kernel_size=5, stride=1, padding=2, bias=True)      # conv1_channels x in_width x in_width\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(pool1_width, stride=2)                                                        # conv1_channels x w2 x w2\n",
    "\n",
    "        w2 = floor((in_width - pool1_width) / 2 + 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, kernel_size=5, stride=1, padding=2, bias=True)   # conv2_channels x w2 x w2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(pool2_width, stride=2)                                                        # conv2_channels x w3 x w3\n",
    "        \n",
    "        w3 = floor((w2 - pool2_width) / 2 + 1)\n",
    "\n",
    "        self.flatten3 = nn.Flatten()                                                                            # (conv2_channels x w3 x w3)\n",
    "        self.fc3 = nn.Linear((int)(conv2_channels * w3 * w3), fc3_width)                                        # fc3width\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(fc3_width, fc4_width)                                                              # fc4width\n",
    "        self.relu4 = nn.ReLU()                                                       \n",
    "\n",
    "        self.fc_logits = nn.Linear(fc4_width, class_count)                                                      # class_count\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear) and m is not self.fc_logits:\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.fc_logits.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.relu1(h)\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu2(h)\n",
    "        h = self.pool2(h)\n",
    "        \n",
    "        h = self.flatten3(h)\n",
    "        h = self.fc3(h)\n",
    "        h = self.relu3(h)\n",
    "\n",
    "        h = self.fc4(h)\n",
    "        h = self.relu4(h)\n",
    "\n",
    "        logits = self.fc_logits(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y5vn1-PxqUj3"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display_markdown\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "\n",
    "    def softmax(self, x):\n",
    "        max = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        x_exp_shifted = torch.exp(x - max)\n",
    "        return x_exp_shifted / torch.sum(x_exp_shifted, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = torch.eye(x.shape[1])[y].cuda()\n",
    "        probs = self.softmax(x)\n",
    "        return -torch.mean(torch.log(torch.sum(probs * y, dim=1)))\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes=10, feat_dim=90):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "        def __init__(self, nll, center_loss, lambd):\n",
    "            super(CombinedLoss, self).__init__()\n",
    "            self.nll = nll\n",
    "            self.center_loss = center_loss\n",
    "            self.lambd = lambd\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            return self.lambd * self.center_loss(x, y) + self.nll(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYH2LVZ8TUFJ",
    "outputId": "3dfb1491-f91c-49a2-e3f0-a0a3f231a517"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "Time elapsed: 0.00 min\n",
      "Train -> Loss: 2.046 | Acc: 25.566%\n",
      "Test -> Loss: 1.699 | Acc: 38.080%\n",
      "\n",
      "Epoch: 1\n",
      "Time elapsed: 0.25 min\n",
      "Train -> Loss: 1.595 | Acc: 41.398%\n",
      "Test -> Loss: 1.491 | Acc: 45.890%\n",
      "\n",
      "Epoch: 2\n",
      "Time elapsed: 0.50 min\n",
      "Train -> Loss: 1.414 | Acc: 48.390%\n",
      "Test -> Loss: 1.289 | Acc: 54.020%\n",
      "\n",
      "Epoch: 3\n",
      "Time elapsed: 0.75 min\n",
      "Train -> Loss: 1.328 | Acc: 52.102%\n",
      "Test -> Loss: 1.238 | Acc: 55.230%\n",
      "\n",
      "Epoch: 4\n",
      "Time elapsed: 1.00 min\n",
      "Train -> Loss: 1.254 | Acc: 55.036%\n",
      "Test -> Loss: 1.229 | Acc: 56.170%\n",
      "\n",
      "Epoch: 5\n",
      "Time elapsed: 1.25 min\n",
      "Train -> Loss: 1.196 | Acc: 57.336%\n",
      "Test -> Loss: 1.146 | Acc: 59.950%\n",
      "\n",
      "Epoch: 6\n",
      "Time elapsed: 1.50 min\n",
      "Train -> Loss: 1.153 | Acc: 58.922%\n",
      "Test -> Loss: 1.122 | Acc: 60.320%\n",
      "\n",
      "Epoch: 7\n",
      "Time elapsed: 1.75 min\n",
      "Train -> Loss: 1.109 | Acc: 60.576%\n",
      "Test -> Loss: 1.041 | Acc: 62.820%\n",
      "\n",
      "Epoch: 8\n",
      "Time elapsed: 2.00 min\n",
      "Train -> Loss: 1.083 | Acc: 61.616%\n",
      "Test -> Loss: 1.035 | Acc: 63.450%\n",
      "\n",
      "Epoch: 9\n",
      "Time elapsed: 2.24 min\n",
      "Train -> Loss: 1.036 | Acc: 63.218%\n",
      "Test -> Loss: 0.988 | Acc: 64.980%\n",
      "\n",
      "Epoch: 10\n",
      "Time elapsed: 2.49 min\n",
      "Train -> Loss: 1.004 | Acc: 64.382%\n",
      "Test -> Loss: 0.960 | Acc: 66.310%\n",
      "\n",
      "Epoch: 11\n",
      "Time elapsed: 2.73 min\n",
      "Train -> Loss: 0.992 | Acc: 64.778%\n",
      "Test -> Loss: 0.946 | Acc: 66.660%\n",
      "\n",
      "Epoch: 12\n",
      "Time elapsed: 2.98 min\n",
      "Train -> Loss: 0.962 | Acc: 66.192%\n",
      "Test -> Loss: 0.936 | Acc: 67.210%\n",
      "\n",
      "Epoch: 13\n",
      "Time elapsed: 3.23 min\n",
      "Train -> Loss: 0.940 | Acc: 66.700%\n",
      "Test -> Loss: 0.901 | Acc: 69.030%\n",
      "\n",
      "Epoch: 14\n",
      "Time elapsed: 3.47 min\n",
      "Train -> Loss: 0.929 | Acc: 67.158%\n",
      "Test -> Loss: 0.904 | Acc: 68.360%\n",
      "\n",
      "Epoch: 15\n",
      "Time elapsed: 3.72 min\n",
      "Train -> Loss: 0.917 | Acc: 67.590%\n",
      "Test -> Loss: 0.933 | Acc: 67.260%\n",
      "\n",
      "Epoch: 16\n",
      "Time elapsed: 3.96 min\n",
      "Train -> Loss: 0.899 | Acc: 68.528%\n",
      "Test -> Loss: 0.918 | Acc: 68.540%\n",
      "\n",
      "Epoch: 17\n",
      "Time elapsed: 4.21 min\n",
      "Train -> Loss: 0.879 | Acc: 69.054%\n",
      "Test -> Loss: 0.883 | Acc: 69.030%\n",
      "\n",
      "Epoch: 18\n",
      "Time elapsed: 4.45 min\n",
      "Train -> Loss: 0.870 | Acc: 69.344%\n",
      "Test -> Loss: 0.862 | Acc: 70.070%\n",
      "\n",
      "Epoch: 19\n",
      "Time elapsed: 4.70 min\n",
      "Train -> Loss: 0.855 | Acc: 70.078%\n",
      "Test -> Loss: 0.877 | Acc: 69.740%\n",
      "\n",
      "Epoch: 20\n",
      "Time elapsed: 4.94 min\n",
      "Train -> Loss: 0.852 | Acc: 69.874%\n",
      "Test -> Loss: 0.877 | Acc: 69.480%\n",
      "\n",
      "Epoch: 21\n",
      "Time elapsed: 5.19 min\n",
      "Train -> Loss: 0.836 | Acc: 70.518%\n",
      "Test -> Loss: 0.852 | Acc: 70.470%\n",
      "\n",
      "Epoch: 22\n",
      "Time elapsed: 5.43 min\n",
      "Train -> Loss: 0.823 | Acc: 71.098%\n",
      "Test -> Loss: 0.823 | Acc: 71.370%\n",
      "\n",
      "Epoch: 23\n",
      "Time elapsed: 5.68 min\n",
      "Train -> Loss: 0.829 | Acc: 71.058%\n",
      "Test -> Loss: 0.841 | Acc: 70.860%\n",
      "\n",
      "Epoch: 24\n",
      "Time elapsed: 5.93 min\n",
      "Train -> Loss: 0.816 | Acc: 71.208%\n",
      "Test -> Loss: 0.805 | Acc: 72.060%\n",
      "\n",
      "Epoch: 25\n",
      "Time elapsed: 6.17 min\n",
      "Train -> Loss: 0.811 | Acc: 71.560%\n",
      "Test -> Loss: 0.819 | Acc: 71.910%\n",
      "\n",
      "Epoch: 26\n",
      "Time elapsed: 6.42 min\n",
      "Train -> Loss: 0.794 | Acc: 72.062%\n",
      "Test -> Loss: 0.827 | Acc: 71.280%\n",
      "\n",
      "Epoch: 27\n",
      "Time elapsed: 6.66 min\n",
      "Train -> Loss: 0.791 | Acc: 72.374%\n",
      "Test -> Loss: 0.811 | Acc: 72.070%\n",
      "\n",
      "Epoch: 28\n",
      "Time elapsed: 6.91 min\n",
      "Train -> Loss: 0.785 | Acc: 72.386%\n",
      "Test -> Loss: 0.822 | Acc: 71.840%\n",
      "\n",
      "Epoch: 29\n",
      "Time elapsed: 7.15 min\n",
      "Train -> Loss: 0.777 | Acc: 72.674%\n",
      "Test -> Loss: 0.817 | Acc: 72.010%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "#Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "#hyperparameters\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "         'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#torch.save(net, 'cnn_cifar.pth')\n",
    "#Model\n",
    "print('==> Building model..')\n",
    "best_acc = 0  \n",
    "start_epoch = 0  \n",
    "device = 'cuda'\n",
    "net = ConvolutionalModel(3, 32, 16, 3, 32, 3, 256, 128, 10)\n",
    "net = net.to(device)\n",
    "\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "alpha = 3e-3\n",
    "alpha_cent = 0.5\n",
    "lambd = 0.01\n",
    "nll = NegativeLogLikelihood()\n",
    "center_loss = CenterLoss(num_classes=10, feat_dim=10)\n",
    "combined_loss = CombinedLoss(nll, center_loss, lambd)\n",
    "criterion = combined_loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=alpha)\n",
    "cent_optimizer = optim.SGD(center_loss.parameters(), lr=alpha_cent)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "start_time = time.time()\n",
    "\n",
    "#Training function\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        for param in center_loss.parameters():\n",
    "            param.grad.data *= (1./lambd)\n",
    "\n",
    "        cent_optimizer.step()\n",
    "        cent_optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print('Train -> Loss: %.3f | Acc: %.3f%%'\n",
    "                  % (train_loss/(len(trainloader)), 100.*correct/total))\n",
    "#Test function\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print('Test -> Loss: %.3f | Acc: %.3f%%'\n",
    "                  % (test_loss/(len(testloader)), 100.*correct/total))\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+30):\n",
    "      train(epoch)\n",
    "      test(epoch)\n",
    "      scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_cifarAdam512.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}