{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Projekt-R-DU/prva-faza/blob/master/Cnn/cnn_mnistAdam128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BZeN9_23TjPd"
   },
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "from torch import nn\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, in_width, conv1_channels, pool1_width, conv2_channels, pool2_width, fc3_width, fc4_width, class_count):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "                                                                                                                # in_channels x in_width x in_width\n",
    "        self.conv1 = nn.Conv2d(in_channels, conv1_channels, kernel_size=5, stride=1, padding=2, bias=True)      # conv1_channels x in_width x in_width\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(pool1_width, stride=2)                                                        # conv1_channels x w2 x w2\n",
    "\n",
    "        w2 = floor((in_width - pool1_width) / 2 + 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, kernel_size=5, stride=1, padding=2, bias=True)   # conv2_channels x w2 x w2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(pool2_width, stride=2)                                                        # conv2_channels x w3 x w3\n",
    "        \n",
    "        w3 = floor((w2 - pool2_width) / 2 + 1)\n",
    "\n",
    "        self.flatten3 = nn.Flatten()                                                                            # (conv2_channels x w3 x w3)\n",
    "        self.fc3 = nn.Linear((int)(conv2_channels * w3 * w3), fc3_width)                                        # fc3width\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(fc3_width, fc4_width)                                                              # fc4width\n",
    "        self.relu4 = nn.ReLU()                                                       \n",
    "\n",
    "        self.fc_logits = nn.Linear(fc4_width, class_count)                                                      # class_count\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear) and m is not self.fc_logits:\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.fc_logits.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.relu1(h)\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu2(h)\n",
    "        h = self.pool2(h)\n",
    "        \n",
    "        h = self.flatten3(h)\n",
    "        h = self.fc3(h)\n",
    "        h = self.relu3(h)\n",
    "\n",
    "        h = self.fc4(h)\n",
    "        h = self.relu4(h)\n",
    "\n",
    "        logits = self.fc_logits(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y5vn1-PxqUj3"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display_markdown\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "\n",
    "    def softmax(self, x):\n",
    "        max = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        x_exp_shifted = torch.exp(x - max)\n",
    "        return x_exp_shifted / torch.sum(x_exp_shifted, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = torch.eye(x.shape[1])[y].cuda()\n",
    "        probs = self.softmax(x)\n",
    "        return -torch.mean(torch.log(torch.sum(probs * y, dim=1)))\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes=10, feat_dim=90):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "        def __init__(self, nll, center_loss, lambd):\n",
    "            super(CombinedLoss, self).__init__()\n",
    "            self.nll = nll\n",
    "            self.center_loss = center_loss\n",
    "            self.lambd = lambd\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            return self.lambd * self.center_loss(x, y) + self.nll(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYH2LVZ8TUFJ",
    "outputId": "6006015c-822c-4a5f-ef92-a6fd559c9321"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "Time elapsed: 0.00 min\n",
      "Train -> Loss: 0.678 | Acc: 77.700%\n",
      "Test -> Loss: 0.254 | Acc: 91.880%\n",
      "\n",
      "Epoch: 1\n",
      "Time elapsed: 0.36 min\n",
      "Train -> Loss: 0.276 | Acc: 91.105%\n",
      "Test -> Loss: 0.157 | Acc: 94.700%\n",
      "\n",
      "Epoch: 2\n",
      "Time elapsed: 0.73 min\n",
      "Train -> Loss: 0.208 | Acc: 93.447%\n",
      "Test -> Loss: 0.131 | Acc: 95.600%\n",
      "\n",
      "Epoch: 3\n",
      "Time elapsed: 1.09 min\n",
      "Train -> Loss: 0.168 | Acc: 94.700%\n",
      "Test -> Loss: 0.129 | Acc: 95.790%\n",
      "\n",
      "Epoch: 4\n",
      "Time elapsed: 1.46 min\n",
      "Train -> Loss: 0.138 | Acc: 95.663%\n",
      "Test -> Loss: 0.099 | Acc: 96.930%\n",
      "\n",
      "Epoch: 5\n",
      "Time elapsed: 1.83 min\n",
      "Train -> Loss: 0.123 | Acc: 96.148%\n",
      "Test -> Loss: 0.093 | Acc: 96.940%\n",
      "\n",
      "Epoch: 6\n",
      "Time elapsed: 2.19 min\n",
      "Train -> Loss: 0.108 | Acc: 96.558%\n",
      "Test -> Loss: 0.067 | Acc: 97.670%\n",
      "\n",
      "Epoch: 7\n",
      "Time elapsed: 2.56 min\n",
      "Train -> Loss: 0.100 | Acc: 96.748%\n",
      "Test -> Loss: 0.065 | Acc: 97.980%\n",
      "\n",
      "Epoch: 8\n",
      "Time elapsed: 2.92 min\n",
      "Train -> Loss: 0.091 | Acc: 97.162%\n",
      "Test -> Loss: 0.062 | Acc: 97.860%\n",
      "\n",
      "Epoch: 9\n",
      "Time elapsed: 3.28 min\n",
      "Train -> Loss: 0.086 | Acc: 97.252%\n",
      "Test -> Loss: 0.066 | Acc: 97.660%\n",
      "\n",
      "Epoch: 10\n",
      "Time elapsed: 3.66 min\n",
      "Train -> Loss: 0.082 | Acc: 97.400%\n",
      "Test -> Loss: 0.063 | Acc: 97.950%\n",
      "\n",
      "Epoch: 11\n",
      "Time elapsed: 4.03 min\n",
      "Train -> Loss: 0.076 | Acc: 97.590%\n",
      "Test -> Loss: 0.061 | Acc: 97.920%\n",
      "\n",
      "Epoch: 12\n",
      "Time elapsed: 4.40 min\n",
      "Train -> Loss: 0.075 | Acc: 97.580%\n",
      "Test -> Loss: 0.051 | Acc: 98.340%\n",
      "\n",
      "Epoch: 13\n",
      "Time elapsed: 4.76 min\n",
      "Train -> Loss: 0.070 | Acc: 97.782%\n",
      "Test -> Loss: 0.054 | Acc: 98.300%\n",
      "\n",
      "Epoch: 14\n",
      "Time elapsed: 5.13 min\n",
      "Train -> Loss: 0.069 | Acc: 97.788%\n",
      "Test -> Loss: 0.056 | Acc: 98.110%\n",
      "\n",
      "Epoch: 15\n",
      "Time elapsed: 5.50 min\n",
      "Train -> Loss: 0.063 | Acc: 98.010%\n",
      "Test -> Loss: 0.056 | Acc: 98.080%\n",
      "\n",
      "Epoch: 16\n",
      "Time elapsed: 5.86 min\n",
      "Train -> Loss: 0.064 | Acc: 97.920%\n",
      "Test -> Loss: 0.064 | Acc: 97.960%\n",
      "\n",
      "Epoch: 17\n",
      "Time elapsed: 6.23 min\n",
      "Train -> Loss: 0.060 | Acc: 98.083%\n",
      "Test -> Loss: 0.045 | Acc: 98.440%\n",
      "\n",
      "Epoch: 18\n",
      "Time elapsed: 6.60 min\n",
      "Train -> Loss: 0.059 | Acc: 98.082%\n",
      "Test -> Loss: 0.058 | Acc: 98.120%\n",
      "\n",
      "Epoch: 19\n",
      "Time elapsed: 6.96 min\n",
      "Train -> Loss: 0.057 | Acc: 98.172%\n",
      "Test -> Loss: 0.051 | Acc: 98.350%\n",
      "\n",
      "Epoch: 20\n",
      "Time elapsed: 7.33 min\n",
      "Train -> Loss: 0.055 | Acc: 98.233%\n",
      "Test -> Loss: 0.050 | Acc: 98.440%\n",
      "\n",
      "Epoch: 21\n",
      "Time elapsed: 7.70 min\n",
      "Train -> Loss: 0.053 | Acc: 98.342%\n",
      "Test -> Loss: 0.048 | Acc: 98.310%\n",
      "\n",
      "Epoch: 22\n",
      "Time elapsed: 8.07 min\n",
      "Train -> Loss: 0.053 | Acc: 98.277%\n",
      "Test -> Loss: 0.051 | Acc: 98.300%\n",
      "\n",
      "Epoch: 23\n",
      "Time elapsed: 8.43 min\n",
      "Train -> Loss: 0.052 | Acc: 98.283%\n",
      "Test -> Loss: 0.049 | Acc: 98.520%\n",
      "\n",
      "Epoch: 24\n",
      "Time elapsed: 8.80 min\n",
      "Train -> Loss: 0.050 | Acc: 98.447%\n",
      "Test -> Loss: 0.041 | Acc: 98.690%\n",
      "\n",
      "Epoch: 25\n",
      "Time elapsed: 9.16 min\n",
      "Train -> Loss: 0.048 | Acc: 98.472%\n",
      "Test -> Loss: 0.048 | Acc: 98.540%\n",
      "\n",
      "Epoch: 26\n",
      "Time elapsed: 9.52 min\n",
      "Train -> Loss: 0.048 | Acc: 98.473%\n",
      "Test -> Loss: 0.057 | Acc: 98.150%\n",
      "\n",
      "Epoch: 27\n",
      "Time elapsed: 9.89 min\n",
      "Train -> Loss: 0.047 | Acc: 98.468%\n",
      "Test -> Loss: 0.042 | Acc: 98.590%\n",
      "\n",
      "Epoch: 28\n",
      "Time elapsed: 10.26 min\n",
      "Train -> Loss: 0.045 | Acc: 98.522%\n",
      "Test -> Loss: 0.041 | Acc: 98.620%\n",
      "\n",
      "Epoch: 29\n",
      "Time elapsed: 10.63 min\n",
      "Train -> Loss: 0.045 | Acc: 98.553%\n",
      "Test -> Loss: 0.043 | Acc: 98.510%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "#Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "#hyperparameters\n",
    "classes = ('0', '1', '2', '3', '4',\n",
    "           '5', '6', '7', '8', '9')\n",
    "\n",
    "#torch.save(net, 'cnn_mnist.pth')\n",
    "#Model\n",
    "print('==> Building model..')\n",
    "best_acc = 0  \n",
    "start_epoch = 0  \n",
    "device = 'cuda'\n",
    "net = ConvolutionalModel(1, 28, 16, 3, 32, 3, 256, 128, 10)\n",
    "net = net.to(device)\n",
    "\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "alpha = 3e-4\n",
    "alpha_cent = 0.5\n",
    "lambd = 0.01\n",
    "nll = NegativeLogLikelihood()\n",
    "center_loss = CenterLoss(num_classes=10, feat_dim=10)\n",
    "combined_loss = CombinedLoss(nll, center_loss, lambd)\n",
    "criterion = combined_loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=alpha)\n",
    "cent_optimizer = optim.SGD(center_loss.parameters(), lr=alpha_cent)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "start_time = time.time()\n",
    "\n",
    "#Training function\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        for param in center_loss.parameters():\n",
    "            param.grad.data *= (1./lambd)\n",
    "\n",
    "        cent_optimizer.step()\n",
    "        cent_optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print('Train -> Loss: %.3f | Acc: %.3f%%'\n",
    "                  % (train_loss/(len(trainloader)), 100.*correct/total))\n",
    "#Test function\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print('Test -> Loss: %.3f | Acc: %.3f%%'\n",
    "                  % (test_loss/(len(testloader)), 100.*correct/total))\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+30):\n",
    "      train(epoch)\n",
    "      test(epoch)\n",
    "      scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_mnistAdam128.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}