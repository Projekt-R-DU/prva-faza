{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoGKx31cSGPu",
        "outputId": "356ebb43-cfec-46b4-970e-7ad6412bd22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'cifar-10-python*': No such file or directory\n",
            "--2021-12-13 22:54:33--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  96.9MB/s    in 1.7s    \n",
            "\n",
            "2021-12-13 22:54:35 (96.9 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-python.tar.gz\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!rm cifar-10-python*\n",
        "!wget \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_fEougSUcw",
        "outputId": "74244605-c5a7-4e11-8bd5-b0ef5e13f5f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "hxFA7vBQTBV0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OYnpx3CZTO5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362b1e22-b828-424b-ccb1-03d637671e54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  cifar-10-python.tar.gz  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_data(data_x, data_y):\n",
        "  indices = np.arange(data_x.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
        "  shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
        "  return shuffled_data_x, shuffled_data_y\n",
        "\n",
        "def unpickle(file):\n",
        "  fo = open(file, 'rb')\n",
        "  dict = pickle.load(fo, encoding='latin1')\n",
        "  fo.close()\n",
        "  return dict\n",
        "\n",
        "DATA_DIR = 'cifar-10-batches-py/'\n",
        "\n",
        "img_height = 32\n",
        "img_width = 32\n",
        "num_channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
        "train_y = []\n",
        "for i in range(1, 6):\n",
        "  subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
        "  train_x = np.vstack((train_x, subset['data']))\n",
        "  train_y += subset['labels']\n",
        "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0, 2, 3, 1)\n",
        "train_y = np.array(train_y, dtype=np.int32)\n",
        "\n",
        "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
        "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0, 2, 3, 1).astype(np.float32)\n",
        "test_y = np.array(subset['labels'], dtype=np.int32)\n",
        "\n",
        "valid_size = 5000\n",
        "train_x, train_y = shuffle_data(train_x, train_y)\n",
        "valid_x = train_x[:valid_size, ...]\n",
        "valid_y = train_y[:valid_size, ...]\n",
        "train_x = train_x[valid_size:, ...]\n",
        "train_y = train_y[valid_size:, ...]\n",
        "data_mean = train_x.mean((0, 1, 2))\n",
        "data_std = train_x.std((0, 1, 2))\n",
        "\n",
        "train_x = (train_x - data_mean) / data_std\n",
        "valid_x = (valid_x - data_mean) / data_std\n",
        "test_x = (test_x - data_mean) / data_std\n",
        "\n",
        "train_x = train_x.transpose(0, 3, 1, 2)\n",
        "valid_x = valid_x.transpose(0, 3, 1, 2)\n",
        "test_x = test_x.transpose(0, 3, 1, 2)\n"
      ],
      "metadata": {
        "id": "xqt3FdjmT_RW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dio dolje iskopiran iz 1. zadatka (Luka)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ayJzCDV3-Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import floor\n",
        "from torch import nn\n",
        "\n",
        "class ConvolutionalModel(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, in_width, conv1_channels, pool1_width, conv2_channels, pool2_width, fc3_width, fc4_width, class_count):\n",
        "        super(ConvolutionalModel, self).__init__()\n",
        "                                                                                                                # in_channels x in_width x in_width\n",
        "        self.conv1 = nn.Conv2d(in_channels, conv1_channels, kernel_size=5, stride=1, padding=2, bias=True)      # conv1_channels x in_width x in_width\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(pool1_width, stride=2)                                                        # conv1_channels x w2 x w2\n",
        "\n",
        "        w2 = floor((in_width - pool1_width) / 2 + 1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, kernel_size=5, stride=1, padding=2, bias=True)   # conv2_channels x w2 x w2\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(pool2_width, stride=2)                                                        # conv2_channels x w3 x w3\n",
        "        \n",
        "        w3 = floor((w2 - pool2_width) / 2 + 1)\n",
        "\n",
        "        self.flatten3 = nn.Flatten()                                                                            # (conv2_channels x w3 x w3)\n",
        "        self.fc3 = nn.Linear((int)(conv2_channels * w3 * w3), fc3_width)                                        # fc3width\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(fc3_width, fc4_width)                                                              # fc4width\n",
        "        self.relu4 = nn.ReLU()                                                       \n",
        "\n",
        "        self.fc_logits = nn.Linear(fc4_width, class_count)                                                      # class_count\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear) and m is not self.fc_logits:\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        self.fc_logits.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv1(x)\n",
        "        h = self.relu1(h)\n",
        "        h = self.pool1(h)\n",
        "\n",
        "        h = self.conv2(h)\n",
        "        h = self.relu2(h)\n",
        "        h = self.pool2(h)\n",
        "        \n",
        "        h = self.flatten3(h)\n",
        "        h = self.fc3(h)\n",
        "        h = self.relu3(h)\n",
        "\n",
        "        h = self.fc4(h)\n",
        "        h = self.relu4(h)\n",
        "\n",
        "        logits = self.fc_logits(h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ujFhnr2JO89H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data_channels = 3\n",
        "data_width = 32\n",
        "CifarCnn = ConvolutionalModel(data_channels, data_width, 16, 3, 32, 3, 256, 128, 10)\n",
        "\n",
        "inp = torch.randn(1, 3, 32, 32)\n",
        "out = CifarCnn(inp)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdvoepLzO-K-",
        "outputId": "f56e4f05-bc16-44cc-ee3a-5b0d16efe017"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6008,  0.1036, -1.0019, -0.0990,  0.4445, -0.7669, -0.8683,  0.6521,\n",
            "         -2.4161,  2.4411]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_values = []\n",
        "\n",
        "for i in valid_y:\n",
        "  appended = np.zeros(10)\n",
        "  appended[i] = 1\n",
        "  valid_values.append(appended)\n",
        "\n",
        "valid_labels = np.array(valid_values)\n",
        "\n",
        "train_values = []\n",
        "\n",
        "for i in train_y:\n",
        "  appended = np.zeros(10)\n",
        "  appended[i] = 1\n",
        "  train_values.append(appended)\n",
        "train_labels = np.array(train_values)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q26nWogSPLTG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYDV0A6A9Mtr",
        "outputId": "f4bf009e-4e84-4de2-9c0f-b5f7d393354d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ],
      "metadata": {
        "id": "3ESHtqsbi1c_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, criterion, x_in, y_in):\n",
        "  # np array u tensor\n",
        "  x = torch.FloatTensor(x_in)\n",
        "  y = torch.FloatTensor(y_in)\n",
        "\n",
        "  outputs = model.forward(x)\n",
        "  loss = criterion(outputs, y)\n",
        "\n",
        "  total = len(y)\n",
        "  temp_list = []\n",
        "  for test in y:\n",
        "    temp_list.append(np.where(test==1)[0][0])\n",
        "\n",
        "  targets_np = np.array(temp_list)\n",
        "  _, predicted_tensor = outputs.max(1)\n",
        "  predicted_np = predicted_tensor.detach().numpy()\n",
        "  correct = np.sum(targets_np==predicted_np)\n",
        "  accuracy = correct / total\n",
        "\n",
        "  return float(loss.detach().numpy()), accuracy"
      ],
      "metadata": {
        "id": "eCZIl-p9oBnI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = {}\n",
        "plot_data['train_loss'] = []\n",
        "plot_data['valid_loss'] = []\n",
        "plot_data['train_acc'] = []\n",
        "plot_data['valid_acc'] = []\n",
        "plot_data['lr'] = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(CifarCnn.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "num_epochs = 5\n",
        "n_batch=500 #bsz izračunato iz duljine trening seta i broja batcheva\n",
        "bsz = len(train_x) // n_batch\n",
        "for epoch in range(num_epochs):\n",
        "    X, Yoh = shuffle_data(train_x, train_labels)\n",
        "    X = torch.FloatTensor(X)\n",
        "    Yoh = torch.FloatTensor(Yoh)\n",
        "    for batch in range(n_batch):\n",
        "        # broj primjera djeljiv s veličinom grupe bsz\n",
        "        batch_X = X[batch*bsz:(batch+1)*bsz, :]\n",
        "        batch_Yoh = Yoh[batch*bsz:(batch+1)*bsz, :]\n",
        "        outputs = CifarCnn.forward(batch_X)\n",
        "\n",
        "        loss = criterion(outputs, batch_Yoh)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch%50 == 0:\n",
        "            print(\"epoch: {}, step: {}/{}, batch_loss: {}\".format(epoch, batch, n_batch, loss))\n",
        "\n",
        "    valid_loss, valid_accuracy = evaluate(CifarCnn, criterion, valid_x, valid_labels)\n",
        "    print(f\"valid_loss:{valid_loss} , valid_accuracy:{valid_accuracy}\")\n",
        "    \n",
        "    lr_scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wh_u3YDQCb9",
        "outputId": "d5efa4ca-f3e8-4e51-b1df-3fb59d04331f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0/500, batch_loss: 2.599403142929077\n",
            "epoch: 0, step: 50/500, batch_loss: 1.773411750793457\n",
            "epoch: 0, step: 100/500, batch_loss: 1.5525628328323364\n",
            "epoch: 0, step: 150/500, batch_loss: 1.6426035165786743\n",
            "epoch: 0, step: 200/500, batch_loss: 1.4936728477478027\n",
            "epoch: 0, step: 250/500, batch_loss: 1.3914496898651123\n",
            "epoch: 0, step: 300/500, batch_loss: 1.3761553764343262\n",
            "epoch: 0, step: 350/500, batch_loss: 1.2882521152496338\n",
            "epoch: 0, step: 400/500, batch_loss: 1.15902578830719\n",
            "epoch: 0, step: 450/500, batch_loss: 1.1230683326721191\n",
            "valid_loss:1.1793975830078125 , valid_accuracy:0.5772\n",
            "epoch: 1, step: 0/500, batch_loss: 1.1701619625091553\n",
            "epoch: 1, step: 50/500, batch_loss: 1.1129745244979858\n",
            "epoch: 1, step: 100/500, batch_loss: 0.8968616127967834\n",
            "epoch: 1, step: 150/500, batch_loss: 1.032361626625061\n",
            "epoch: 1, step: 200/500, batch_loss: 1.1367759704589844\n",
            "epoch: 1, step: 250/500, batch_loss: 0.9920256733894348\n",
            "epoch: 1, step: 300/500, batch_loss: 0.9115530848503113\n",
            "epoch: 1, step: 350/500, batch_loss: 1.0202794075012207\n",
            "epoch: 1, step: 400/500, batch_loss: 0.9788334965705872\n",
            "epoch: 1, step: 450/500, batch_loss: 0.8326486945152283\n",
            "valid_loss:1.0216866731643677 , valid_accuracy:0.6442\n",
            "epoch: 2, step: 0/500, batch_loss: 0.8630321025848389\n",
            "epoch: 2, step: 50/500, batch_loss: 0.8877650499343872\n",
            "epoch: 2, step: 100/500, batch_loss: 0.6747440695762634\n",
            "epoch: 2, step: 150/500, batch_loss: 0.7983038425445557\n",
            "epoch: 2, step: 200/500, batch_loss: 1.0597625970840454\n",
            "epoch: 2, step: 250/500, batch_loss: 0.5381187200546265\n",
            "epoch: 2, step: 300/500, batch_loss: 0.939820408821106\n",
            "epoch: 2, step: 350/500, batch_loss: 1.0203485488891602\n",
            "epoch: 2, step: 400/500, batch_loss: 0.8578906655311584\n",
            "epoch: 2, step: 450/500, batch_loss: 0.9861472249031067\n",
            "valid_loss:0.954003095626831 , valid_accuracy:0.6684\n",
            "epoch: 3, step: 0/500, batch_loss: 0.822125256061554\n",
            "epoch: 3, step: 50/500, batch_loss: 0.7986020445823669\n",
            "epoch: 3, step: 100/500, batch_loss: 0.7596011757850647\n",
            "epoch: 3, step: 150/500, batch_loss: 0.7737233638763428\n",
            "epoch: 3, step: 200/500, batch_loss: 0.7637863159179688\n",
            "epoch: 3, step: 250/500, batch_loss: 0.9587277173995972\n",
            "epoch: 3, step: 300/500, batch_loss: 1.055840253829956\n",
            "epoch: 3, step: 350/500, batch_loss: 0.8798273801803589\n",
            "epoch: 3, step: 400/500, batch_loss: 0.7397168278694153\n",
            "epoch: 3, step: 450/500, batch_loss: 0.8297151327133179\n",
            "valid_loss:0.9178916215896606 , valid_accuracy:0.6878\n",
            "epoch: 4, step: 0/500, batch_loss: 0.6183142066001892\n",
            "epoch: 4, step: 50/500, batch_loss: 0.7880009412765503\n",
            "epoch: 4, step: 100/500, batch_loss: 0.7364938259124756\n",
            "epoch: 4, step: 150/500, batch_loss: 0.7532308101654053\n",
            "epoch: 4, step: 200/500, batch_loss: 0.7740363478660583\n",
            "epoch: 4, step: 250/500, batch_loss: 0.4188741147518158\n",
            "epoch: 4, step: 300/500, batch_loss: 0.9791056513786316\n",
            "epoch: 4, step: 350/500, batch_loss: 0.7169432640075684\n",
            "epoch: 4, step: 400/500, batch_loss: 0.7065339088439941\n",
            "epoch: 4, step: 450/500, batch_loss: 0.7546721696853638\n",
            "valid_loss:0.8956167697906494 , valid_accuracy:0.6978\n"
          ]
        }
      ]
    }
  ]
}